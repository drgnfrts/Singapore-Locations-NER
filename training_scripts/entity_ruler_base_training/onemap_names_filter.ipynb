{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Name Filter\n",
    "Processing of locations scraped from OneMap. Code is written to extract relevant names based on the raw data format in data/singapore-postal-codes. Import first JSON, then a function to read the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_path = '../data/singapore-postal-codes/'\n",
    "\n",
    "def load_data(file):\n",
    "    with open(data_path + file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to properly capitalise location names is placed here to facilitate cleaning location names for all values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to properly capitalise and clean values\n",
    "def properly_capitalise(location_name):\n",
    "    properly_capitalised_name = location_name.lower().title()\n",
    "    return properly_capitalised_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRT/LRT Station Names Extraction\n",
    "Function created to extract station names, based on the raw format of the json they came in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract mrt & lrt station names\n",
    "def stn_name_filter(data):\n",
    "    # filters out \"possible locations\" by zooming in on sub-list\n",
    "    list_of_stns= []\n",
    "    for i in range(len(data)):\n",
    "        list_of_stns.append(json.dumps(data[i][\"Possible Locations\"]))\n",
    "\n",
    "    # filters out actual station names (14 index after \"SEARCHVAL\": \" to \", \"X\":) in a loop\n",
    "    count = 0\n",
    "    for each_stn in list_of_stns:  \n",
    "        if each_stn != []:\n",
    "            final_name = each_stn[each_stn.find('\"SEARCHVAL\": \"') + 14:each_stn.find('\", \"X\":')]\n",
    "            #replace original entry with actual station name\n",
    "            list_of_stns[count] = final_name.lower()\n",
    "        count += 1\n",
    "    #filters out blank strings\n",
    "    while ('' in list_of_stns):\n",
    "        list_of_stns.remove('')\n",
    "    \n",
    "    return list_of_stns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction, updating and cleaning of LRT station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load LRT station names and append missing stations\n",
    "lrt_data0 = load_data(\"lrt_stations.json\")\n",
    "uncleaned_lrt_stns = stn_name_filter(lrt_data0) + ['senja lrt station', 'sengkang lrt station', 'punggol lrt station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Properly capitalise LRT station names and add to list\n",
    "lrt_stns = []\n",
    "for station in uncleaned_lrt_stns:\n",
    "    lrt_stn_name = properly_capitalise(station).replace(\"Lrt\", \"LRT\")\n",
    "    lrt_stns.append(lrt_stn_name)\n",
    "    lrt_no_stn = lrt_stn_name.replace(\" LRT Station\", '')\n",
    "    lrt_stns.append(lrt_no_stn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction, updating and cleaning of MRT station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MRT station names and append missing stations\n",
    "mrt_data0 = load_data(\"mrt_stations.json\")\n",
    "#append missing station names\n",
    "uncleaned_mrt_stns = stn_name_filter(mrt_data0) + ['woodlands north mrt station', 'woodlands south mrt station', 'springleaf mrt station', 'lentor mrt station', 'mayflower mrt station', 'bright hill mrt station', 'upper thomson mrt station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Properly capitalise MRT station names and add to list\n",
    "mrt_stns = []\n",
    "for station in uncleaned_mrt_stns:\n",
    "    mrt_stn_name = properly_capitalise(station).replace(\"Mrt\", \"MRT\")\n",
    "    mrt_stns.append(mrt_stn_name)\n",
    "    mrt_no_stn = mrt_stn_name.replace(\" MRT Station\", '')\n",
    "    mrt_stns.append(mrt_no_stn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Names Extraction\n",
    "\n",
    "From buildings.json, several categories of location values were selected that are meaningful to everyday identification of building locations. The values for POSTAL, BUILDING, ROAD_NAME and BLK_NO were picked out, correctly capitalised and put together to give the following list of values:\n",
    "\n",
    "SIMPLE ADDRESS - created from the cleaned values of BLK_NO and ROAD_NAME  \n",
    "POSTCODE - created by concat of \"Singapore\" to POSTAL  \n",
    "BUILDING NAME - created by cleaning BUILDING  \n",
    "ROAD NAME - created by cleaning ROAD NAME\n",
    "\n",
    "These lists are segragated to enable future provision to filter them as seperate entity tags, rather than a singular \"LOC\". For now, they will be concat together into a single JSON file.\n",
    "\n",
    "Previously, SIMPLE ADDRESS was created by taking SEARCHVAL, then deleting the postcode \"SINGAPORE XXXXXX\" via RegEx and building name if duplicated. However, this process was not precise enough - certain private estate names were input as the building name in the raw data, which result in filtering out even road names from SIMPLE ADDRESS. This left the filtered data with entries that are only numbers, which messed up the EntityRuler later on in the NER creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_data0 = load_data(\"buildings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating three seperate lists for buildingname, address and postcode. not sure if it is meaningful to split but doing so for now if we need it in the future.\n",
    "buildings_name_list = []\n",
    "buildings_address_list = []\n",
    "buildings_postcode = []\n",
    "road_names_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(value):\n",
    "    value_result = properly_capitalise(json.dumps(buildings_data0[i][value], sort_keys = True).strip('\"'))\n",
    "    return value_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through each item in the scraped buildings list\n",
    "for i in range(len(buildings_data0)):\n",
    "    onemap_postal = \"Singapore \" + extract_value(\"POSTAL\")\n",
    "    buildings_postcode.append(onemap_postal)\n",
    "\n",
    "    onemap_building = extract_value('BUILDING')\n",
    "    buildings_name_list.append(onemap_building)\n",
    "    \n",
    "    onemap_road_name = extract_value('ROAD_NAME')\n",
    "    road_names_list.append(onemap_road_name)\n",
    "\n",
    "    onemap_blk_no = extract_value(\"BLK_NO\")\n",
    "    simple_buildings_address = onemap_blk_no + \" \" + onemap_road_name\n",
    "    buildings_address_list.append(simple_buildings_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove duplicate items in list\n",
    "def duplicate_remover(thelist):\n",
    "    org_length = len(thelist)\n",
    "    the_new_list = sorted(list(set(thelist)))\n",
    "    final_length = len(the_new_list)\n",
    "    print(f\"Number of unique items reduced from {org_length} to {final_length}\")\n",
    "    return the_new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items reduced from 141726 to 16165\n",
      "Number of unique items reduced from 141726 to 132757\n",
      "Number of unique items reduced from 141726 to 121361\n",
      "Number of unique items reduced from 141726 to 3867\n"
     ]
    }
   ],
   "source": [
    "#putting unique items into their finalised lists\n",
    "new_buildings_name_list = duplicate_remover(buildings_name_list)\n",
    "new_buildings_address_list = duplicate_remover(buildings_address_list)\n",
    "new_buildings_postcode = duplicate_remover(buildings_postcode)\n",
    "new_road_names_list = duplicate_remover(road_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer of Data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_path = \"../../data/extracted_locations/\"\n",
    "\n",
    "def save_data(file, data):\n",
    "    with open (save_data_path + file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "save_data('extracted_mrt_stns.json', mrt_stns)\n",
    "\n",
    "save_data('extracted_lrt_stns.json', lrt_stns)\n",
    "\n",
    "save_data('extracted_buildings_address_list.json', new_buildings_address_list)\n",
    "\n",
    "save_data('extracted_buildings_name_list.json', new_buildings_name_list)\n",
    "\n",
    "save_data('extracted_buildings_postcode.json', new_buildings_postcode)\n",
    "\n",
    "save_data('extracted_road_names_list.json', new_road_names_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26f8a204728a0623e20edacd624cb2c042787b92acd36e3318b98c359b8d3657"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('spacytestenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
