{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN_DATA generator\n",
    "\n",
    "Here Wikipedia and news articles are passed through the EntityRuler-enhanced model (loc_er) created earlier to generate the training data in spaCy v2 format in an automated manner.\n",
    "\n",
    "Import the following modules: WikipediaAPI, spaCy, newspaper3k and its Article function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import spacy\n",
    "import json\n",
    "import newspaper\n",
    "import random\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: The model required is in models/loc_er, which is not directly linked to this folder. Replace the path below with the full path to models/loc_er.\n",
    "#path = os.path.realpath(r\"C:\\Users\\nicol\\OneDrive\\Desktop\\python_stuff\\spacytestenv_in_vscode\\models\\loc_er\")\n",
    "path = \"..\\..\\models\\loc_er\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the EntityRuler-enhanced model from earlier. On my laptop CPU this takes 2.5 mins on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Wikipedia API as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the Wikipedia API and inputting language and format \n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of articles to be processed by the EntityRuler-enhanced models. This is a small sample collection, and placing the article names/url into a list enables future provision for the list to be stored in a seperate JSON file to avoid clogging up the main ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_newspaper_urls = [\n",
    "    \"https://www.channelnewsasia.com/singapore/police-arrest-man-clementi-police-centre-gunshot-2505181\",\n",
    "    \"https://www.channelnewsasia.com/singapore/flood-heavy-rain-weather-warnings-fallen-trees-vehicles-punggol-sungei-gedong-2509816\",\n",
    "    \"https://www.straitstimes.com/singapore/community/integrated-community-hub-one-punggol-to-open-from-mid-2022-700-seat-hawker\",\n",
    "    \"https://www.channelnewsasia.com/singapore/fire-breaks-out-jurong-east-condominium-1-taken-hospital-2388701\",\n",
    "    \"https://mothership.sg/2022/01/jurong-east-choked-garbage-chute/\",\n",
    "    \"https://www.straitstimes.com/singapore/housing/hougang-bto-flats-draw-more-than-10000-applicants-all-seven-projects\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_wikipedia_articles = [\n",
    "    \"North South MRT line\", \n",
    "    \"East West MRT line\", \n",
    "    \"Downtown MRT line\", \n",
    "    \"Mass Rapid Transit (Singapore)\", \n",
    "    \"Geography of Singapore\", \n",
    "    \"Transport in Singapore\", \n",
    "    'Pan Island Expressway', \n",
    "    \"Urban planning in Singapore\", \n",
    "    \"Future developments in Singapore\", \n",
    "    \"North East MRT line\", \n",
    "    \"Chinatown MRT station\", \n",
    "    \"Circle MRT line\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing of text article by the EntityRuler-enhanced model below. A large function (generate_annotations) calls the following \"sub-functions\":  \n",
    "1. Pulling of the article from Wikipedia or the news website (extract_wikipedia / extract_news_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract only the text portion from the article, and clean up any syntax issues.\n",
    "def extract_wikipedia(article):\n",
    "    p_wiki = wiki_wiki.page(article)\n",
    "    wikitext = p_wiki.text\n",
    "    if \"\\n\" in wikitext:\n",
    "        wikitext = wikitext.replace(\"\\n\", \"\")\n",
    "    return wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract news articles. The articles are downloaded and parsed before the text is returned.\n",
    "def extract_news_articles(article_url):\n",
    "    news_article = Article(article_url)\n",
    "    news_article.download()\n",
    "    news_article.parse()\n",
    "    return news_article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Breaking the text into sentences (break_into_sentences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to break the article into sentences to make it easier for the NLP model to process. The list of sentences is typically referred to as a \"corpus\", hence the odd naming.\n",
    "def break_into_sentences(article):\n",
    "    corpus = []\n",
    "    doc = nlp(article)\n",
    "    for sent in doc.sents:\n",
    "        corpus.append(sent.text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sentences are passed into the EntityRuler-enhanced model and annotated in spaCy v2 traindata format (generate_traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_traindata(text):\n",
    "    doc = nlp(text)\n",
    "    results = []\n",
    "    locations = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"LOC\":\n",
    "            locations.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    if len(locations) > 0:\n",
    "        results = [text, {\"entities\": locations}]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 functions are called in the large function, and the results are appended to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_annotations(article, type):\n",
    "    extracted_values = []\n",
    "    if type == \"Wikipedia Article\":\n",
    "        extracted_article = extract_wikipedia(article)\n",
    "    elif type == \"News Article\":\n",
    "        extracted_article = extract_news_articles(article)\n",
    "    cleaned_text = break_into_sentences(extracted_article)\n",
    "    for sentences in cleaned_text:\n",
    "        results = generate_traindata(sentences)\n",
    "        if results != None:\n",
    "            extracted_values.append(results)\n",
    "    return extracted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final list of annotations is generated by running the list of news and wikipedia articles through the generate_annotations function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924 sets of annotations were created.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = []\n",
    "for articles in list_of_wikipedia_articles:\n",
    "    list_of_annotations = generate_annotations(articles, \"Wikipedia Article\")\n",
    "    for items in list_of_annotations:\n",
    "        TRAIN_DATA.append(items)\n",
    "\n",
    "for articles in list_of_newspaper_urls:\n",
    "    list_of_annotations = generate_annotations(articles, \"News Article\")\n",
    "    for items in list_of_annotations:\n",
    "        TRAIN_DATA.append(items)\n",
    "\n",
    "print(f'{len(TRAIN_DATA)} sets of annotations were created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "923 461 462\n"
     ]
    }
   ],
   "source": [
    "final_index = len(TRAIN_DATA) - 1\n",
    "mid_index = int(final_index / 2)\n",
    "valid_index_start = mid_index + 1\n",
    "\n",
    "print(final_index, mid_index, valid_index_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(TRAIN_DATA)\n",
    "training_set = TRAIN_DATA[0:mid_index]\n",
    "validation_set = TRAIN_DATA[valid_index_start:final_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then saved to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_DATA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12932/1206683856.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msave_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_datasets/full_train_data_v1.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_DATA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msave_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_datasets/training_set.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msave_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_datasets/validation_set.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TRAIN_DATA' is not defined"
     ]
    }
   ],
   "source": [
    "save_data_path = \"../../data/training_datasets/\"\n",
    "\n",
    "def save_data(file, data):\n",
    "    with open (save_data_path + file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "save_data(\"full_train_data_er.json\", TRAIN_DATA)\n",
    "save_data(\"training_set_er.json\", training_set)\n",
    "save_data(\"validation_set_er.json\", validation_set)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26dff31311a0bce229de49839ffd0c16c131302b688e22273eb8197e58305d33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('spacyenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
