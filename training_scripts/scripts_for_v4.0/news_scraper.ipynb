{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for Doccano Annotations\n",
    "\n",
    "Doccano accepts input text in the form of JSON, text or textline files. For plain, unedited text, textline format seems to be the easiest method to import a large number of datasets to Doccano at one go. Textline is basically a text file, but each new line is processed by Doccano as a new dataset.\n",
    "\n",
    "This notebook scrapes text from Wikipedia and news articles, breaking them up into sentences, and sending each sentence to a new line in the text file.\n",
    "\n",
    "Import the following modules: WikipediaAPI, newspaper3k with its Article function and spaCy's en_core_wen_sm model (to parse text into sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the newspaper3k API as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract news articles. The articles are downloaded and parsed before the text is returned.\n",
    "def extract_news_articles(article_url):\n",
    "    news_article = Article(article_url)\n",
    "    news_article.download()\n",
    "    news_article.parse()\n",
    "    return news_article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the main functions used to scrape and break the text into sentences.\n",
    "\n",
    "The primary function is *concat_all_sentences()*, which consists of a nested function *articles_to_sentences()*, which further calls *breaks_into_sentences()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup NLP pipeline to break the article into sentences\n",
    "\n",
    "def break_into_sentences(article):\n",
    "    corpus = []\n",
    "    doc = nlp(article)\n",
    "    for sent in doc.sents:\n",
    "        corpus.append(sent.text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what kind of type of article the input article is, then use the relevant API to scrape and call break_into_sentences() to return a list of sentences\n",
    "\n",
    "def article_to_sentences(article, article_type):\n",
    "    if article_type == \"Wikipedia Article\":\n",
    "        extracted_article = extract_wikipedia(article)\n",
    "    elif article_type == \"News Article\":\n",
    "        extracted_article = extract_news_articles(article)\n",
    "    cleaned_text = break_into_sentences(extracted_article)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_for_doccano = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the primary function to scrape articles from a given list, break them up into sentences, and concats the sentences into a list for transfer to a text file. The function requires two parameter inputs - list of articles and article type.\n",
    "\n",
    "sentences_for_doccano = []\n",
    "def concat_all_sentences(list_of_articles, article_type):\n",
    "    for article in list_of_articles:\n",
    "        list_of_sentences = article_to_sentences(article, article_type)\n",
    "        for sentences in list_of_sentences:\n",
    "            sentences_for_doccano.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_newspaper_urls = [\n",
    "    \"https://www.channelnewsasia.com/sustainability/ocbc-sustainability-energy-efficient-technology-25-million-investment-2705961\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_all_sentences(list_of_newspaper_urls, \"News Article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../data/text_data/news_and_wiki_data.txt\", \"w\") as f:\n",
    "    for each_sentence in sentences_for_doccano:\n",
    "        f.write(each_sentence + \"\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99dd627970ccf7a76dbbe014bbf707e817867099f13e2ab9e56b37d735cfd9d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
